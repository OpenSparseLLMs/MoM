# -*- coding: utf-8 -*-

from fla.layers import (ABCAttention, Attention, BasedLinearAttention,
                        BitAttention, DeltaNet, GatedDeltaNet, MomGatedDeltaNet, GatedLinearAttention,
                        GatedSlotAttention, HGRN2Attention, HGRNAttention,
                        LinearAttention, MomLinearAttention, MultiScaleRetention,
                        ReBasedLinearAttention, MomGatedSlotAttention,
                        MomGatedLinearAttention)
from fla.models import (ABCForCausalLM, ABCModel, BitNetForCausalLM,
                        BitNetModel, DeltaNetForCausalLM, DeltaNetModel,
                        GatedDeltaNetForCausalLM, GatedDeltaNetModel,
                        MomGatedDeltaNetForCausalLM, MomGatedDeltaNetModel,
                        GLAForCausalLM, GLAModel, GSAForCausalLM, GSAModel,
                        HGRN2ForCausalLM, HGRN2Model, HGRNForCausalLM,
                        LinearAttentionForCausalLM, LinearAttentionModel,
                        MomLinearAttentionForCausalLM, MomLinearAttentionModel,
                        RetNetForCausalLM, RetNetModel, RWKV6ForCausalLM,
                        RWKV6Model, TransformerForCausalLM, TransformerModel,
                        MomGLAForCausalLM, MomGLAModel,
                        MomGSAForCausalLM, MomGSAModel)

__all__ = [
    'ABCAttention',
    'Attention',
    'BasedLinearAttention',
    'BitAttention',
    'DeltaNet',
    'GatedDeltaNet',
    'MomGatedDeltaNet',
    'HGRNAttention',
    'HGRN2Attention',
    'GatedLinearAttention',
    'MomGatedLinearAttention',
    'GatedSlotAttention',
    'MomGatedSlotAttention',
    'LinearAttention',
    'MomLinearAttention',
    'MultiScaleRetention',
    'ReBasedLinearAttention',
    'ABCForCausalLM',
    'ABCModel',
    'BitNetForCausalLM',
    'BitNetModel',
    'DeltaNetForCausalLM',
    'DeltaNetModel',
    'GatedDeltaNetForCausalLM',
    'GatedDeltaNetModel',
    'MomGatedDeltaNetForCausalLM',
    'MomGatedDeltaNetModel',
    'HGRNForCausalLM',
    'HGRNModel',
    'HGRN2ForCausalLM',
    'HGRN2Model',
    'GLAForCausalLM',
    'GLAModel',
    'MomGLAForCausalLM',
    'MomGLAModel',
    'GSAForCausalLM',
    'GSAModel',
    'MomGSAForCausalLM',
    'MomGSAModel',
    'LinearAttentionForCausalLM',
    'LinearAttentionModel',
    'MomLinearAttentionForCausalLM',
    'MomLinearAttentionModel',
    'RetNetForCausalLM',
    'RetNetModel',
    'RWKV6ForCausalLM',
    'RWKV6Model',
    'TransformerForCausalLM',
    'TransformerModel',
    'chunk_gla',
    'chunk_retention',
    'fused_chunk_based',
    'fused_chunk_gla',
    'fused_chunk_retention'
]

__version__ = '0.1'
